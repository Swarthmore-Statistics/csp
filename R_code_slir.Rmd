---
title: "Code for Statistical Learning in R"
author: "Kelly McConville"
date: "February 15, 2018"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, cache = TRUE, tidy = TRUE, eval = FALSE, tidy.opts = list(blank = FALSE, width.cutoff = 50))
```

This document contains all of the R code for the course *Statistical Learning in R* at the Conference on the Statistical Practice 2018.

```{r}
## ---------  Load necessary libraries --------------

#Data wrangling
library(dplyr)

#Plots
library(ggplot2)
#Set theme options
theme_set(theme_bw(base_size = 9)) 

#kable tables
library(knitr)

#Elastic net
library(glmnet)

#Tidy output
library(broom)

#Predictive models
library(caret)

#Scatterplot matrices
library(GGally)

#Data Import
library(readr)

#Regression Splines
library(splines)

#Smoothing Splines
library(SemiPar)

#SVM
library(kernlab)

#Classification Trees
library(rpart)

#Plots for Trees
library(partykit)


## ---------  Read in data ----------------------------------

#Household level data
ce <- read_csv("fmli164.csv", na = c("NA", "."))

#Member level data
memi <- read_csv("memi164.csv", na = c("NA", "."))


## --------- Wrangle data ----------------------------------

#Merge in columns from memi level data
ce$PRINEARN <- as.integer(ce$PRINEARN)

#Add Info about Principal Earner
ce <- select(memi, AGE, SEX, MEMBRACE, HORIGIN, NEWID, MEMBNO) %>%
  right_join(ce, by = c("NEWID" = "NEWID", "MEMBNO" = "PRINEARN"))

#Convert appropriate variables to factors
ce <- mutate(ce, POPSIZE = factor(POPSIZE), CUTENURE = factor(CUTENURE), BLS_URBN = factor(BLS_URBN),
             MARITAL1 = factor(MARITAL1), HIGH_EDU = factor(HIGH_EDU), FAM_TYPE = factor(FAM_TYPE), SEX = 
               factor(SEX), MEMBRACE = factor(MEMBRACE), HORIGIN = factor(HORIGIN)) %>% 
#Create log transformed versions of several right skewed variables 
  mutate(logIRAX = log(IRAX), logFINCBTAX = log(FINCBTAX), logTOTEXPCQ = log(TOTEXPCQ))


## ---- Model Fitting Example ---------------------


#Set random number generator
set.seed(3322)

#Generate data
x <- runif(n = 15, min = 0,max = 4)
y <-  -1*(x - 2)^3 + 3*(x - 2) + 2  + rnorm(n = 15,0,1)
#Store data in dataframe
dat <- data.frame(x,y)


## Plot possible models
ggplot(dat, aes(x = x,y = y))  + stat_smooth(method = "lm", formula = y ~ x, size = 1, se = FALSE) + stat_smooth(method = "lm", formula = y ~ poly(x, 2), se = FALSE, color = "olivedrab") + stat_smooth(method = "lm", formula = y ~ poly(x, 3), se = FALSE, color = "orange") + stat_smooth(method = "lm", formula = y ~ poly(x, 8), se = FALSE, color = "salmon")  + ylim(c(-.5, 5.5)) + geom_point(x = 3.25, y = 3.75, size = 4, color = "red") + geom_point(size = 3) 


## -------- CV Example ----------
#Graph data 
filter(ce, TOTEXPCQ > 0, FINCBTAX > 0 ) %>%
ggpairs(columns = c("logTOTEXPCQ", "logFINCBTAX", "AGE", "SEX"), axisLabels = "none", lower = list(combo = "box", continuous = "cor"), upper = list(continuous = "points"), columnLabels = c("Logged Expenses", "Logged Income", "Age", "Sex")) 

#Restrict attention to households with positive expenditures and income
dat <-  ce %>%
  filter(TOTEXPCQ > 0, FINCBTAX > 0) %>%
  select(logTOTEXPCQ, logFINCBTAX, AGE, SEX) 

#Fit models
mod1 <- lm(logTOTEXPCQ ~ poly(logFINCBTAX, degree = 2), 
           data = dat)
mod2 <- lm(logTOTEXPCQ ~ logFINCBTAX + AGE + SEX, data = dat)
mod3 <- lm(logTOTEXPCQ ~ logFINCBTAX*SEX*AGE, data = dat)

#Predict
pred1 <- predict(object = mod1)
pred2 <- predict(object = mod2)
pred3 <- predict(object = mod3)

#Leverage values
h1 <- hat(model.matrix(mod1))
h2 <- hat(model.matrix(mod2))
h3 <- hat(model.matrix(mod3))

#Test MSE from LOOCV
test_mse1 <- mean(((dat$logTOTEXPCQ - pred1)/(1 - h1))^2)
test_mse2 <- mean(((dat$logTOTEXPCQ - pred2)/(1 - h2))^2)
test_mse3 <- mean(((dat$logTOTEXPCQ - pred3)/(1 - h3))^2)


## Create table of MSE ratios
kable(cbind(test_mse1/test_mse1,test_mse2/test_mse1,test_mse3/test_mse1), col.names = c("Model 1", "Model 2", "Model 3"), caption = "Ratio of Test MSE to MSE of Model 1")

## ------ Penalized Least Squares: Elastic Net -----------------

## ---- Wrangle Data -----------------------------
# Pick Y and X's
ds <- filter(ce, TOTEXPCQ > 0, FINCBTAX > 0) %>%
  dplyr::select(logTOTEXPCQ, FAM_SIZE, BLS_URBN, AS_COMP1, 
                AS_COMP2, AS_COMP4, AS_COMP5, logFINCBTAX,
                HIGH_EDU, ROOMSQ, SEX, AGE,
                MEMBRACE, BEDROOMQ) %>%
  # Remove missing values
  na.omit()
  

## --------- glmnet ------------------------
# Create design matrix
x <- model.matrix(logTOTEXPCQ ~ ., data = ds)[,-1]

# Define response variable
y <- ds$logTOTEXPCQ

# Fit Lasso model
fit <- glmnet(x, y, alpha = 1, standardize = TRUE, nlambda = 100)


# Look at coefficient paths
plot(fit, xvar = "lambda", label = TRUE)


# Use cross-validation to pick lambda
cvfits <- cv.glmnet(x, y, alpha = 1, standardize = TRUE, nfolds = 10)
glance(cvfits)

#Plot cv results
plot(cvfits)

#Look at coefficient paths
plot(fit, xvar = "lambda", label = TRUE)
abline(v = log(cvfits$lambda.1se))
abline(v = log(cvfits$lambda.min))


# Coefficients
tidy(coef(object = cvfits, s = "lambda.min"))[,-2]
tidy(coef(object = cvfits, s = "lambda.1se"))[,-2]

## --------- glmnet using caret to Tune alpha and lambda ---------

#Set-up grid of possible lambda and alpha values
lam <- 10^seq(-4, -2, length.out = 20)
alpha <- 0:10/10
grd <- expand.grid(lambda = lam, alpha = alpha)

#Set-up CV options
cv_opts <- trainControl(method = "repeatedcv", number = 10, 
                        repeats = 5)

#Train glmnet model
cvfits2 <- train(logTOTEXPCQ ~ ., data = ds, 
                 method = "glmnet", tuneGrid = grd,
                 trControl = cv_opts, standardize = TRUE)

#Plot cv results
plot(cvfits2)

#Best hyperparameters
cvfits2$bestTune

#Best model
bestfit2 <- cvfits2$finalModel
#Coefficients
tidy(coef(bestfit2, s = cvfits2$bestTune$lambda))[,-2]

## Adaptive glmnet in caret
# Penalty weights
pen <- abs(lm(y~scale(x))$coef[-1])^(-1)

#Set-up grid of possible lambda and alpha values
lam <- 10^seq(-4, -.5, length.out = 20)
alpha <- 0:10/10
grd <- expand.grid(lambda = lam, alpha = alpha)

#Train glmnet model with differing penalties
cvfits3 <- train(logTOTEXPCQ ~ ., data = ds, 
                 method = "glmnet", tuneGrid = grd, 
                 trControl = cv_opts, standardize = TRUE, 
                 penalty.factor = pen)
#Plot cv results
plot(cvfits3)
#Best hyperparameters
cvfits3$bestTune
#Best model
bestfit3 <- cvfits3$finalModel
#Coefficients
tidy(coef(bestfit3, s = cvfits3$bestTune$lambda))[,-2]

## ---- Non-Parametric Models -----------------------------

#Filter out those without any retirement funds
dat <- dplyr::filter(ce, IRAX > 0)

# Plot data
ggplot(dat, aes(x = AGE, y = IRAX)) + geom_point(alpha = .6) + stat_smooth(method="lm", se = FALSE) + labs(x = "Age", y = "Amount in Retirement Account \n (in $)")

# Report top coded observations
dat <- dplyr::filter(dat, IRAX != max(ce$IRAX, na.rm = TRUE))

# Plot data
ggplot(dat, aes(x = AGE, y = IRAX)) + geom_point(alpha = .6) + stat_smooth(method = "lm", se = FALSE, size = 2) + labs(x = "Age", y = "Amount in Retirement Account (in $)")

# Look at linear model
mod <- lm(IRAX~AGE, data = dat)
kable(tidy(mod))

# Add squared term to linear model
ggplot(dat, aes(x = AGE, y = IRAX)) + geom_point(alpha = .6) + stat_smooth(method = "lm", se = FALSE, size = 2, formula = y ~ poly(x, 2, raw = TRUE), color = "orange") + labs(x = "Age", y = "Amount in Retirement Account (in $)")
mod2 <- lm(IRAX~poly(AGE, 2), data = dat)
kable(tidy(mod2))
mod_tidy <- tidy(mod2)

# Add cubic term to linear model
ggplot(dat, aes(x = AGE, y = IRAX)) + geom_point(alpha = .6) + stat_smooth(method = "lm", se = FALSE, size = 2, formula=y ~ poly(x, 3, raw = TRUE), color = "red") + labs(x = "Age", y = "Amount in Retirement Account (in $)")
mod3 <- lm(IRAX ~ poly(AGE, 3), data = dat)
kable(tidy(mod3))
mod_tidy <- tidy(mod3)

# Step function example
ggplot(dat, aes(x = AGE, y = IRAX)) + geom_point(alpha = .6) + stat_smooth(method = "lm", se = FALSE, size = 2, formula=y ~ cut(x, breaks=1:10*10), color="blue") + labs(x = "Age", y = "Amount in Retirement Account (in $)")

# Linear basis
x <- seq(0, 1, by=0.001)
plot(x,x, type='l', lwd=2, col=1, xlab="Linear 
Basis", ylab="", ylim=c(0,1))
abline(h=1, lwd=2) 

# Cubic polynomial basis
plot(x,x, type='l', lwd=2, col = 1, xlab = "Cubic Polynomial 
Basis", ylab="", ylim = c(0,1))
abline(h = 1, lwd = 2)
lines(x, x^2, lwd = 2)
lines(x, x^3, lwd = 2)

## ------------ Regression splines -----------------------

## Piece-wise polynomials with no smoothness constraints
ggplot(dat, aes(x=AGE, y=IRAX)) + geom_vline(xintercept =2:4*20, color="orange", size = 1.5,linetype="longdash") + geom_point(alpha = .6) + stat_smooth(method="lm", se = FALSE, size = 2, formula=y ~ poly(x,degree=3)*cut(x, breaks=c(min(x)-1,2:4*20)), color="blue") + labs(x = "Age", y = "Amount in Retirement Account (in $)")

#Truncated power functions
tp <- function(x, t, p){
  (x - t) ^ p * (x > t)
}
plot(x,tp(x,0,1), type="l", xlab="Linear Truncated Bases", ylab="", lwd=2 )
abline(h=1, lwd=2) 
lines(x,tp(x,.2,1), lwd = 2)
lines(x,tp(x,.4,1), lwd = 2)
lines(x,tp(x,.6,1), lwd = 2)
lines(x,tp(x,.8,1), lwd = 2)


## Linear B-spline basis
x <- seq(0, 1, by=0.001)
spl.bs <- bs(x,df=5, degree = 1)
plot(x, spl.bs[,1], ylim=c(0,1), type='l', lwd=2, xlab="Linear B-spline basis", ylab="")
lines(x, spl.bs[,2], lwd=2)
lines(x, spl.bs[,3], lwd=2)
lines(x, spl.bs[,4], lwd=2)

## Regression splines with B splines
ggplot(dat, aes(x = AGE, y = IRAX)) + geom_vline(xintercept =c(min(dat$AGE),2:5*20), color="orange", size = 1.5,linetype="longdash") + geom_point() + stat_smooth(method = "lm", se = FALSE, size = 2, formula=y ~ bs(x, knots=c(40,60,80)), color="blue")  + labs(x = "Age", y = "Amount in Retirement Account (in $)")

# Regression splines with natural splines
ggplot(dat, aes(x = AGE, y = IRAX)) + geom_vline(xintercept =c(min(dat$AGE),2:5*20), color="orange", size = 1.5,linetype="longdash") + geom_point() + stat_smooth(method = "lm", se = FALSE, size = 2, formula=y ~ ns(x, knots=c(40,60,80)), color="blue")  + labs(x = "Age", y = "Amount in Retirement Account (in $)")

# --------------- Smoothing Splines -----------------

# Fit smoothing splines model
mod <- smooth.spline(x = dat$AGE, y = dat$IRAX, cv=TRUE)
dat2 <- data.frame(x=mod$x, fits = mod$y)

#Graph smoothing splines
ggplot(dat, aes(x = AGE, y = IRAX)) + geom_point() + geom_line(data = dat2, aes(x=x,y=fits), color="blue", size = 2)


# Comparing non-parametric and parametric fits
ggplot(dat, aes(x = AGE, y = IRAX)) + geom_point()  + geom_line(data = dat2, aes(x=x,y=fits,color="a1", linetype="a1"), size = 2) + stat_smooth(method = "lm", se = FALSE, size = 2, formula=y ~ poly(x, 3, raw=TRUE), aes(color="a3", linetype="a3")) +   stat_smooth(method = "lm", se = FALSE, size = 2, formula=y ~ poly(x, 6, raw=TRUE), aes(color="a2", linetype="a2"))  + labs(x = "Age", y = "Amount in Retirement Account (in $)") + coord_cartesian(ylim = c(0, 300000)) + scale_linetype_manual(name='Methods',values=c("a3"="dotdash", "a2"="dashed", "a1"="solid"), labels = c('Cubic Polynomial','Degree 6 Polynomial','Degree 5.7 Smoothing Spline')) + scale_colour_manual(name = 'Methods', values =c("a3"="olivedrab3","a2"="darkorchid3","a1"="darkgoldenrod4"), labels = c('Cubic Polynomial','Degree 6 Polynomial','Degree 5.7 Smoothing Spline')) + theme(legend.position = c(.25,.8))


#Need CV for cubic polynomial model
mod_poly3 <- mod3 <- lm(IRAX~poly(AGE,3), data = dat)
pred_poly3 <- predict(object = mod_poly3)
h_poly3 <- hat(model.matrix(mod_poly3))
test_mse_poly3 <- mean(((dat$IRAX - pred_poly3)/(1 - h_poly3))^2)

## ------ Classification ------------------------------------

## -------  Data Wrangling --------------------------------

#Create dataset
ds <- dplyr::select(ce, IRAX, TOTEXPCQ, FAM_SIZE, BLS_URBN, AS_COMP1, AS_COMP2,
                FINCBTAX, HIGH_EDU, ROOMSQ, AS_COMP4, AS_COMP5,
                SEX, AGE, BEDROOMQ) %>%
  #Create variable for whether or not have money in IRAX
  mutate(IRAX_cat = ifelse(IRAX > 0, "Yes", "No"))  %>%
  # Remove missing values
  na.omit() %>%
  #Remove IRAX (so we don't accidentally use it as a predictor!)
  select(-IRAX)

## ----------- Logistic Regression --------------------------

#Standard way of fitting logistic model
mod_log <- glm(as.factor(IRAX_cat) ~ ., data = ds, family = "binomial")

#But summary() doesn't have the CV Accuracy
summary(mod_log)

# -------------- Logistic Regression using caret for accuracy --------------
#Set-up CV options
cv_opts <- trainControl(method = "repeatedcv", number = 10, repeats = 5)

#Build logistic regression model
mod_log <- train(IRAX_cat~., data = ds, method = "glm", trControl = cv_opts, family = "binomial")
mod_log


## -------------- Logistic glmnet using caret -------------------

#Set-up grid of possible lambda and alpha values
lam <- 10^seq(-2,-5, length.out = 20)
alpha <- 0:10/10
grd <- expand.grid(lambda = lam, alpha = alpha)

#Train the Logistic Elastic Net Model
cv_en <- train(IRAX_cat~., data = ds, method = "glmnet", tuneGrid = grd, trControl = cv_opts, family = "binomial", standardize = TRUE)

# Results
plot(cv_en)
cv_en$bestTune
bestfit_en <- cv_en$finalModel
tidy(coef(bestfit_en, s = cv_en$bestTune$lambda))[,-2]

## ---------- Logistic adaptive glmnet using caret -----------------

#Create penalty weights
x <- scale(model.matrix(IRAX_cat~., data = ds)[,-1])
pen_log <- abs(glm(as.factor(ds$IRAX_cat)~x, 
                   family = "binomial")$coef)^(-1)

#Train the Logistic Adaptive Elastic Net Model
cv_ena <- train(IRAX_cat~., data = ds, method = "glmnet", tuneGrid = grd, trControl = cv_opts, family="binomial", penalty.factor = pen_log, standardize = TRUE)


#Comparing accuracy
#Logistic Regression 
mod_log$results$Accuracy

#Elastic Net Logistic Regression
max(cv_en$results$Accuracy)

#Adaptive Elastic Net Logistic Regression
max(cv_ena$results$Accuracy)

## ---------------- SVM ---------------------

## ----------SVM toy example ----------------

#Generate dataset
set.seed(106)
n <- 30
x1 <- c(runif(n = n/3,min =0, max=2.5 ), runif(n = 2*n/3,min =3.25, max=6 ))
x2 <- 10 - 2*x1 + rnorm(n, sd = 5)
#To ensure they are on the correct margin lines
x2[10] <- -9.4 + 5 + 5.5*x1[10]
x2[7] <- -9.4 + 5 + 5.5*x1[7]
x2[18] <- -9.4 - 5 + 5.5*x1[18]
y <- c(rep("yes", length.out=n/3), rep("no", length.out=2*n/3))
dat <- data.frame(x1,x2,y)

#Define color palette
cols <- c("#000000", "#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7", "#999999")

#Graph data with possible decision boundaries
ggplot(dat, aes(x1,x2, color=y)) + geom_point(size = 2) + guides(color=FALSE) + labs(x = "X1", y = "X2") + geom_abline(intercept=-5, slope =3, size = 1,color=cols[4] ) + geom_abline(intercept=-6, slope =4, size = 1, color=cols[5]) + geom_abline(intercept=-9, slope =6, size = 1,color=cols[6]) 

# Graph data with decision boundaries and margins
ggplot(dat, aes(x1,x2, color=y)) + geom_segment(aes(x = c(2.7), y = c(10.2), xend = c(3.55), yend = c(5)), color=cols[4]) + geom_point(size = 2) + guides(color=FALSE) + labs(x = "X1", y = "X2") + geom_abline(intercept=-9.4, slope =5.5, size = 1,color=cols[1] ) + geom_abline(intercept=-9.4 -5, slope =5.5, size = 1,color=cols[9], linetype=2 ) + geom_abline(intercept=-9.4 +5, slope =5.5, size = 1,color=cols[9], linetype=2 ) +geom_text(x=3, y=10, label="M", col=cols[4], size = 2) +geom_text(x=3.4, y=7.5, label="M", col=cols[4], size = 2)

# Graph data with decision boundaries and support vectors
ggplot(dat, aes(x1,x2, color=y)) + geom_point(size = 2) + guides(color=FALSE) + labs(x = "X1", y = "X2") + geom_abline(intercept=-9.4, slope =5.5, size = 1,color=cols[1] ) + geom_abline(intercept=-9.4 -5, slope =5.5, size = 1,color=cols[9], linetype=2 ) + geom_abline(intercept=-9.4 +5, slope =5.5, size = 1,color=cols[9], linetype=2 ) + geom_point(data = dat[c(10,7,18), 1:2], color=cols[1], size = 3)

#Adding non-separable points to dataset
x1 <- c(x1, 2.3, 2.75, .8)
x2 <- c(x2, 5.1, -3, 10)
y <- c(y, "yes", "yes", "no")
dat <- data.frame(x1,x2,y)

#Graph data
ggplot(dat, aes(x1,x2, color=y)) + geom_point(size = 2) + guides(color=FALSE) + labs(x = "X1", y = "X2") #+ geom_abline(intercept=-9.4, slope =5.5, size = 1,color=cols[1] ) #+ geom_abline(intercept=-9.4 -5, slope =5.5, size = 1,color=cols[9], linetype=2 ) + geom_abline(intercept=-9.4 +5, slope =5.5, size = 1,color=cols[9], linetype=2 ) + geom_point(data = dat[c(10,7,18), 1:2], color=cols[1], size = 3)  


# Graph data with decision boundary and xi terms
ggplot(dat, aes(x1,x2, color=y)) + geom_segment(aes(x = 2.3, y = 5.1, xend = 2., yend = 6.75), color=cols[2]) + geom_segment(aes(x = 2.75, y = -3, xend = 1.5, yend = 4), color=cols[2]) + geom_segment(aes(x = .8, y = 10, xend = 2.6, yend = 0), color=cols[2]) + geom_abline(intercept=-9.4, slope =5.5, size = .5,color=cols[1] ) + geom_abline(intercept=-9.4 -5, slope =5.5, size = .5,color=cols[9], linetype=2 ) + geom_abline(intercept=-9.4 +5, slope =5.5, size = .5,color=cols[9], linetype=2 ) + geom_point(size = 2) + guides(color=FALSE) + labs(x = "X1", y = "X2") 
 
# Create data that require a non-linear decision boundary
n <- 60
x1 <- c(runif(n = n,min =0, max=6 ))
x2 <- c(15 - 2*x1[1:25] + rnorm(25, sd = 2), 3*x1[26:60] + rnorm(35, sd = 3))
y <- c(rep("yes", length.out=25), rep("no", length.out=35))
dat2 <- data.frame(x1,x2,y)

#Graph data
ggplot(dat2, aes(x1,x2, color=y)) + geom_point(size = 2) + guides(color=FALSE) + labs(x = "X1", y = "X2") 

## ---- SVM with caret---------------------------

## ------ SVM with linear kernal -------------
#Create design matrix
#Selected subset of *useful* predictors 
x <- model.matrix(IRAX_cat~ AGE + FINCBTAX + TOTEXPCQ + HIGH_EDU, data = ds)[,-1]

#Set-up CV options
cv_opts <- trainControl(method = "CV", number = 5)

#Range of values for hyper-parameter
C <- c(.1, 1, 2, 10, 20)

#Train Linear SVM model
cv_svm_l <- train(x, ds$IRAX_cat, method="svmLinear", preProc=c("center", "scale"), tuneGrid = data.frame(C), trControl = cv_opts)
cv_svm_l



## ------ SVM with radial kernal -------------

#Hyper-parameter grid
sig <- sigest(x)
C <- c(.1, .3, .5, 1, 2)
grd <- expand.grid(.sigma=sig, .C=C)


#Train Radial SVM Model
#Use grd OR specify tuneLength and caret will pick possibly tuning values
cv_svm_r <- train(x, ds$IRAX_cat, method="svmRadial", preProc = c("center", "scale"), tuneLength = 10, trControl = cv_opts)
cv_svm_r




## ------Classification Trees ----------------------------

#Build classification tree with one predictor
mod_t <- rpart(IRAX_cat ~ FINCBTAX, data = ds)

#Plot tree
plot(mod_t)
text(mod_t, pretty = 0, use.n  = TRUE, all = TRUE, cex = .8)

# Visual the partition
ds1 <- mutate(ds, Income = ifelse(FINCBTAX < 58100, "High", "Low"))
ggplot(data = ds1, aes(x = FINCBTAX, y = IRAX_cat)) + geom_point(aes(color = Income), position = position_jitter(width = 0, height = .2), alpha = .2) + geom_vline(xintercept = 58100)

#Build classification tree with two predictors
mod_t <- rpart(IRAX_cat ~ FINCBTAX + HIGH_EDU, data = ds, control = rpart.control(cp = .00995))

#Plot tree
plot(mod_t)
text(mod_t, pretty = 0, use.n  = TRUE, all = TRUE, cex = .9)

#Build classification tree
mod_t <- rpart(IRAX_cat ~ ., data = ds)
mod_t

#Fancy plot of tree
plot(as.party(mod_t), gp = gpar(fontsize = 6))

## ----Classification Trees with caret ------------------------------

#Hyper-parameter grid
grd <- data.frame(.cp=(1:50)*0.01)

#Train Classification Tree
cv_tree <- train(IRAX_cat~ . , data = ds, method="rpart", tuneGrid = grd, trControl = cv_opts)
cv_tree$bestTune

cv_tree


#Fit the tree using specified hyper-parameter
mod_t <- rpart(IRAX_cat ~ ., data = ds, control =
                 rpart.control(cp = cv_tree$bestTune))

#Plot the tree
plot(as.party(mod_t))



```

